# Regressions

#### Построение регрессий и базовая демонстрация градиентного спуска

При нажатии ЛКМ добавляется точка в декартовом пространстве (совпадает с координатами пикселей).
Серая кривая - "идеальная регрессия", построенная по формулам.
Черная кривая стремится к ней, изменяясь согласно градиентному спуску.

### Компиляция
Единственная зависимость - библиотека raylib
```console
$ g++ -std=c++17 main.cpp -lraylib -o main
$ ./main
```

### Вычисления
Для вычисления ошибки некоторой кривой $y = f(x)$ используется квадратичная ошибка - сумма квадратов разностей значения функции и $y$ точки из датасета:

$$E = \sum_{i = 0}^{n-1} (f(x_i) - y_i)^2$$

Для минимизации ошибки можно применить метод градиентного спуска. 
Это вовсе не является эффективным методом нахождения уравнения регрессии, однако является базовой демонстрацией одного из самых важных понятий машинного обучения.
Найдем частные производные ошибки относительно каждого параметра функции-регрессии.
Эти частные производные составляют градиентный вектор, показывающий направление возрастания ошибки. Рассмотрим на примере линейной регрессии.

**Уравнение регрессии**
$$y = ax + b$$
**Квадратичная ошибка**
$$E = \sum_{i = 0}^{n-1} (ax_i + b - y_i)^2$$
**Частные производные**
$$\frac{\partial E}{\partial a} = 2(ax_i + b - y_i)x$$

$$\frac{\partial E}{\partial b} = 2(ax_i + b - y_i)$$

**Градиентный вектор**
```math
\Delta E = [2(ax_i + b - y_i)x, 2(ax_i + b - y_i)]
```
Таким образом, для градиентного спуска необходимо сделать смещение на обратный вектор $-\Delta E$.
При этом "конечную" регрессию можно найти решив уравнение $-\Delta E = [0, 0]$, поскольку идеальной считается регрессия, для которой невозможно улучшить значение квадратичной ошибки.
Для линейной регрессии, поделив на 2 и распределя множители, получим:

$$\begin{cases}
\sum ax_i^2 + bx_i - y_ix_i = 0 \\ 
\sum ax_i + b - y_i = 0
\end{cases}
$$

Воспользуемся коммутативностью сложения:

$$\begin{cases}
\sum ax_i^2 + \sum bx_i - \sum y_ix_i = 0 \\ 
\sum ax_i + \sum b - \sum y_i = 0
\end{cases}
$$

И вынесем переменные:

$$\begin{cases}
a\sum x_i^2 + b\sum x_i - \sum y_ix_i = 0 \\ 
a\sum x_i + b\sum 1 - \sum y_i = 0
\end{cases}
$$

Дальше просто решим систему линейных уравнений и получим:

$$
a = \frac{n \sum x_iy_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2}
$$

$$
b = \frac{\sum y_i - a\sum x_i }{n}
$$
